<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>nlp - 标签 - 个人博客</title>
        <link>https://huozhixue.github.io/tags/nlp/</link>
        <description>nlp - 标签 - 个人博客</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 21 Jun 2020 17:49:40 &#43;0800</lastBuildDate><atom:link href="https://huozhixue.github.io/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>后 BERT 时代小结</title>
    <link>https://huozhixue.github.io/nlp-bert_family/</link>
    <pubDate>Sun, 21 Jun 2020 17:49:40 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://huozhixue.github.io/nlp-bert_family/</guid>
    <description><![CDATA[<p>2018年10月，Google 发布 BERT 模型，然后 BERT 横扫了 NLP 的各个任务：
文本分类、序列标注、文本摘要、信息检索、问答系统、阅读理解等等。</p>
<p>到如今，BERT 及其变种几乎一统了 NLU（Natural Language Understanding，自然语言理解）领域，
只有 GPT 模型依托 NLG（Natural Language Generation，自然语言生成）领域能与之周旋。
而细究下来，GPT 与 BERT 的结构其实只有一处不同。</p>
<p>不夸张地说，这是属于 BERT 的时代。因此，本文对 BERT 及其之后的发展作一个粗略的小结，以供参考。</p>]]></description>
</item></channel>
</rss>
